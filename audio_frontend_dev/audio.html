<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Azure OpenAI Realtime Session</title>
    <style>
        body {
            font-family: "Segoe UI", Tahoma, sans-serif;
            margin: 0;
            padding: 2rem;
            background: #f5f7fb;
            color: #1d1d1f;
        }

        h1 {
            margin-top: 0;
        }

        button {
            cursor: pointer;
            border: none;
            border-radius: 6px;
            background-color: #2563eb;
            color: #fff;
            font-size: 1rem;
            padding: 0.6rem 1.2rem;
            transition: background 0.2s ease;
        }

        button:disabled {
            background-color: #9ca3af;
            cursor: not-allowed;
        }

        button:not(:disabled):hover {
            background-color: #1e4ecb;
        }

        .controls-card {
            margin-top: 1.5rem;
            padding: 1.5rem;
            border-radius: 12px;
            background: #ffffff;
            box-shadow: 0 10px 25px rgba(15, 23, 42, 0.08);
            max-width: 720px;
        }

        .controls-card h2 {
            margin-top: 0;
            margin-bottom: 0.75rem;
            font-size: 1.25rem;
        }

        #textPromptForm {
            display: flex;
            gap: 0.75rem;
            align-items: center;
        }

        #textPromptForm input[type="text"] {
            flex: 1;
            padding: 0.7rem 0.9rem;
            border-radius: 8px;
            border: 1px solid #d1d5db;
            font-size: 1rem;
        }

        #textPromptForm input[type="text"]:disabled {
            background: #f3f4f6;
        }

        .helper-text {
            margin-top: 0.5rem;
            color: #6b7280;
            font-size: 0.9rem;
        }

        #logContainer {
            margin-top: 2rem;
            padding: 1rem;
            border-radius: 12px;
            background: #ffffff;
            box-shadow: inset 0 0 0 1px rgba(148, 163, 184, 0.2);
            max-width: 1400px;
            max-height: 800px;
            overflow-y: auto;
        }

        #logContainer p {
            margin: 0.25rem 0;
            font-family: "Consolas", "Courier New", monospace;
            font-size: 0.95rem;
            color: #0f172a;
        }
    </style>
</head>
<body>
    <h1>Azure OpenAI Realtime Session</h1>
    <p>WARNING: Don't use this code sample in production with the API key hardcoded. Use a protected backend service to call the sessions API and generate the ephemeral key. Then return the ephemeral key to the client.</p>
    <button onclick="StartSession()">Start Session</button>
    <div class="controls-card">
        <h2>Send a text message</h2>
        <form id="textPromptForm">
            <label class="sr-only" for="textPrompt">Message</label>
            <input type="text" id="textPrompt" placeholder="Ask the assistant anything" />
            <button id="sendTextButton" type="submit" disabled>Send</button>
        </form>
        <p class="helper-text">Tip: Press Enter to send while the input box is focused.</p>
    </div>

    <!-- Log container for API messages -->
    <div id="logContainer"></div> 

    <script>
        const system_prompt = `## Role & Objective

    You are **Contoso Telco Virtual Agent**, a friendly, fast, and knowledgeable customer-service representative.
    **Goal:** Quickly understand the caller’s issue, retrieve accurate information or take action using available tools, and leave the customer feeling heard and satisfied.

    ---

    ## Personality & Tone

    * **Personality:** Warm, upbeat, empathetic, professional.
    * **Tone:** Friendly and concise. Never robotic or overly formal.
    * **Length:** 2–3 sentences per turn.
    * **Pacing:** Speak at a natural but brisk pace. Respond promptly after the user finishes speaking.

    ---

    ## Language

    * Mirror the caller’s language if intelligible.
    * If the language is unclear, politely default to English.
    * Stay in a single language per call.
    * For this region, the preferred langauges are Arabic and English.

    ---

    ## Conversation Flow

    **Greeting → Discover → Verify → Diagnose/Resolve → Confirm/Close**

    ### Greeting

    Goal: Welcome caller and invite their issue.
    Sample phrases (vary):

    * “Hi, thanks for calling Contoso Telco. How can I help you today?”
    * “Welcome to Contoso Support. What can I look into for you?”

    ### Discover

    * Listen carefully and identify the main issue category.
    * Summarize the issue back for confirmation.

    ### Verify

    * Politely gather necessary account info (e.g., phone number, last bill date) before tool calls.

    ### Diagnose / Resolve

    * Use appropriate tools (see **Tools** section).
    * Provide clear, short updates while working.
    * If an action succeeds, confirm next steps.

    ### Confirm / Close

    * Summarize what was done and ask if anything else is needed.
    * End with a friendly goodbye.

    ---

    ## Tools

    When a tool call is needed, **always** say a short preamble like “I’m checking that now” before calling.
    Tools available (call exact tool name):

    * 'get_billing_info' – Retrieve recent bills, charges, or disputes.
    * 'check_network_connectivity' – Test or report connectivity issues.
    * 'check_service_outage' – Look up area-wide outages.
    * 'get_account_balance' – Provide balance or data usage.
    * 'modify_plan' – Change or upgrade/downgrade plans.
    * 'manage_sim' – Activate/replace SIM, provide PUK.
    * 'process_payment' – Take or confirm payments/recharges.
    * 'device_support' – Troubleshoot phones, routers, or modems.
    * 'schedule_installation' – New service setup or appointments.
    * 'manage_roaming' – Enable/disable roaming, troubleshoot abroad.
    * 'manage_value_added' – Add/remove optional services/features.
    * 'update_account_info' – Change contact details or reset password.
    * 'report_lost_stolen' – Suspend/blacklist lost or stolen devices.
    * 'cancel_service' – Terminate or port service.
    * 'general_info' – Provide coverage details, promotions, FAQs.

    ---

    ## Instructions / Rules

    * **ONLY RESPOND TO CLEAR AUDIO OR TEXT.**
    * If input is noisy or unclear, ask for clarification:

      * “Sorry, I didn’t catch that—could you say it again?”
      * “There’s some background noise. Could you repeat the last part?”
    * Keep each response short and conversational.
    * DO NOT repeat the same sentence twice; vary phrasing.
    * Confirm critical actions (e.g., service cancellation, SIM replacement) before executing.

    ---

    ## Safety & Escalation

    Escalate immediately if:

    * Caller explicitly requests a human.
    * Two failed tool attempts on the same task.
    * Threats, harassment, or safety concerns.
      Say: “Thanks for your patience—I’m connecting you with a specialist now,” then call 'escalate_to_human'.

    ---

    ## Variety

    * Rotate greeting and confirmation phrases to avoid sounding robotic.
    * Use natural filler words sparingly (“Alright,” “Great,” “Let’s see”).

    ---

    ## Reference Pronunciations (sample)

    * PUK: spell as “P-U-K code.”
    * Roaming: pronounce as “roh-ming.”

    ---
    `;
        const CLIENT_CONFIG = {
            backendBaseUrl: "https://ca-audio-backend-gxajhwpdgu6h4.kindcliff-10d999fa.swedencentral.azurecontainerapps.io/api", // Update if your backend runs elsewhere
            deployment: "gpt-realtime",
            voice: "verse",
        };

        let cachedTools = null;
        let toolChoice = "auto";
        const sessionState = {
            dataChannel: null,
            peerConnection: null,
            clientMedia: null,
        };
        const textForm = document.getElementById("textPromptForm");
        const textInput = document.getElementById("textPrompt");
        const sendButton = document.getElementById("sendTextButton");

        textForm.addEventListener("submit", (event) => {
            event.preventDefault();
            sendTextMessage();
        });

        setTextInputEnabled(false);

        async function StartSession() {
            try {
                await ensureToolsLoaded();
                const session = await requestSession();

                logMessage("Ephemeral Key Received: ***");
                logMessage(`WebRTC Session Id = ${session.sessionId}`);

                await initSession({
                    sessionId: session.sessionId,
                    ephemeralKey: session.ephemeralKey,
                    webrtcUrl: session.webrtcUrl,
                    tools: cachedTools,
                    toolChoice,
                });
            } catch (error) {
                console.error("Failed to start session", error);
                logMessage(`Error starting session: ${error.message}`);
            }
        }

        async function ensureToolsLoaded() {
            if (cachedTools !== null) {
                return;
            }

            const response = await fetch(`${CLIENT_CONFIG.backendBaseUrl}/tools`);
            if (!response.ok) {
                throw new Error(`Unable to retrieve tool definitions (${response.status})`);
            }

            const data = await response.json();
            cachedTools = data.tools ?? [];
            toolChoice = data.tool_choice ?? "auto";

            logMessage(`Loaded ${cachedTools.length} tool definition(s)`);
        }

        async function requestSession() {
            const response = await fetch(`${CLIENT_CONFIG.backendBaseUrl}/session`, {
                method: "POST",
                headers: {
                    "Content-Type": "application/json",
                },
                body: JSON.stringify({
                    deployment: CLIENT_CONFIG.deployment,
                    voice: CLIENT_CONFIG.voice,
                }),
            });

            if (!response.ok) {
                const details = await response.text();
                throw new Error(`Session request failed: ${details}`);
            }

            const payload = await response.json();
            return {
                sessionId: payload.session_id,
                ephemeralKey: payload.ephemeral_key,
                webrtcUrl: payload.webrtc_url,
            };
        }

        async function initSession({ sessionId, ephemeralKey, webrtcUrl, tools, toolChoice }) {
            logMessage(`Negotiating WebRTC connection for session ${sessionId}`);

            let peerConnection = new RTCPeerConnection();
            const audioElement = document.createElement('audio');
            audioElement.autoplay = true;
            document.body.appendChild(audioElement);

            peerConnection.ontrack = (event) => {
                audioElement.srcObject = event.streams[0];
            };

            const clientMedia = await navigator.mediaDevices.getUserMedia({ audio: true });
            const audioTrack = clientMedia.getAudioTracks()[0];
            peerConnection.addTrack(audioTrack);

            const dataChannel = peerConnection.createDataChannel('realtime-channel');

            sessionState.dataChannel = dataChannel;
            sessionState.peerConnection = peerConnection;
            sessionState.clientMedia = clientMedia;

            dataChannel.addEventListener('open', () => {
                logMessage('Data channel is open');
                sendSessionUpdate(dataChannel, tools, toolChoice);
                setTextInputEnabled(true);
            });

            dataChannel.addEventListener('message', async (event) => {
                const realtimeEvent = JSON.parse(event.data);
                console.log(realtimeEvent);

                if (realtimeEvent.type === "session.error") {
                    logMessage(`Error: ${realtimeEvent.error.message}`);
                } else if (realtimeEvent.type === "session.end") {
                    logMessage("Session ended.");
                } else if (realtimeEvent.type === "response.output_text.delta") {
                    logMessage(`Text delta: ${realtimeEvent.delta}`);
                } else if (realtimeEvent.type === "response.output_text.done") {
                    logMessage("Text response complete.");
                } else if (realtimeEvent.type === "response.output_audio_transcript.delta") {
                    logMessage(`Transcript delta: ${realtimeEvent.delta}`);
                } else if (realtimeEvent.type === "response.done") {
                    await handleResponseDone(realtimeEvent, dataChannel);
                } else {
                    logMessage(`Received server event: ${JSON.stringify(realtimeEvent)}`);
                }
            });

            dataChannel.addEventListener('close', () => {
                logMessage('Data channel is closed');
            });

            const offer = await peerConnection.createOffer();
            await peerConnection.setLocalDescription(offer);

            const sdpResponse = await fetch(`${webrtcUrl}?model=${encodeURIComponent(CLIENT_CONFIG.deployment)}`, {
                method: "POST",
                body: offer.sdp,
                headers: {
                    Authorization: `Bearer ${ephemeralKey}`,
                    "Content-Type": "application/sdp",
                },
            });

            if (!sdpResponse.ok) {
                throw new Error(`WebRTC negotiation failed (${sdpResponse.status})`);
            }

            const answer = { type: "answer", sdp: await sdpResponse.text() };
            await peerConnection.setRemoteDescription(answer);

            const button = document.createElement('button');
            button.innerText = 'Close Session';
            button.onclick = stopSession;
            document.body.appendChild(button);

            function stopSession() {
                if (dataChannel.readyState === "open") {
                    dataChannel.close();
                }
                if (peerConnection) {
                    peerConnection.getSenders().forEach(sender => sender.track?.stop());
                    peerConnection.close();
                }
                clientMedia.getTracks().forEach(track => track.stop());
                audioElement.srcObject = null;
                button.disabled = true;
                peerConnection = null;
                sessionState.dataChannel = null;
                sessionState.peerConnection = null;
                sessionState.clientMedia = null;
                setTextInputEnabled(false);
                logMessage("Session closed.");
            }
        }

        function sendSessionUpdate(dataChannel, tools, toolChoiceValue) {
            const sessionPayload = {
                type: "session.update",
                session: {
                    instructions: system_prompt, //"You are a helpful AI assistant responding in natural, engaging language.",
                    // output_modalities: ["text", "audio"],
                    input_audio_transcription: {
                       model: "whisper-1"
                    }
                },
            };

            if (Array.isArray(tools) && tools.length > 0) {
                sessionPayload.session.tools = tools;
                sessionPayload.session.tool_choice = toolChoiceValue ?? "auto";
            }

            dataChannel.send(JSON.stringify(sessionPayload));
            logMessage(`Sent session.update: ${JSON.stringify(sessionPayload)}`);
        }

        async function handleResponseDone(event, dataChannel) {
            const response = event.response;
            if (!response || !Array.isArray(response.output)) {
                return;
            }

            for (const item of response.output) {
                if (item.type === "function_call") {
                    await fulfillFunctionCall(item, dataChannel);
                }
            }
        }

        async function fulfillFunctionCall(functionCallItem, dataChannel) {
            const callId = functionCallItem.call_id;
            const functionName = functionCallItem.name;
            const argumentsPayload = functionCallItem.arguments ?? {};

            logMessage(`Model requested function: ${functionName}`);

            try {
                const response = await fetch(`${CLIENT_CONFIG.backendBaseUrl}/function-call`, {
                    method: "POST",
                    headers: {
                        "Content-Type": "application/json",
                    },
                    body: JSON.stringify({
                        name: functionName,
                        call_id: callId,
                        arguments: argumentsPayload,
                    }),
                });

                if (!response.ok) {
                    const detail = await response.text();
                    throw new Error(`Backend error (${response.status}): ${detail}`);
                }

                const result = await response.json();
                sendFunctionCallOutput(dataChannel, callId, result.output);
                logMessage(`Provided output for ${functionName}`);
            } catch (error) {
                console.error(`Function ${functionName} failed`, error);
                const errorPayload = { error: error.message };
                sendFunctionCallOutput(dataChannel, callId, errorPayload);
                logMessage(`Function ${functionName} failed: ${error.message}`);
            }
        }

        function sendFunctionCallOutput(dataChannel, callId, output) {
            const conversationEvent = {
                type: "conversation.item.create",
                item: {
                    type: "function_call_output",
                    call_id: callId,
                    output: JSON.stringify(output),
                },
            };

            dataChannel.send(JSON.stringify(conversationEvent));
            dataChannel.send(JSON.stringify({ type: "response.create" }));
            logMessage(`Sent function_call_output for call ${callId}`);
        }

        function sendTextMessage() {
            const dataChannel = sessionState.dataChannel;
            if (!dataChannel || dataChannel.readyState !== "open") {
                logMessage("Data channel is not ready. Start the session first.");
                return;
            }

            const text = textInput.value.trim();
            if (!text) {
                logMessage("Please enter a message before sending.");
                return;
            }

            const conversationEvent = {
                type: "conversation.item.create",
                item: {
                    type: "message",
                    role: "user",
                    content: [
                        {
                            type: "input_text",
                            text,
                        },
                    ],
                },
            };

            dataChannel.send(JSON.stringify(conversationEvent));
            dataChannel.send(
                JSON.stringify({
                    type: "response.create",
                    // response: {
                    //     output_modalities: ["audio"], //["text", "audio"],
                    // },
                })
            );

            logMessage(`Sent user text: ${text}`);
            textInput.value = "";
        }

        function setTextInputEnabled(isEnabled) {
            textInput.disabled = !isEnabled;
            sendButton.disabled = !isEnabled;
        }

        function logMessage(message) {
            const logContainer = document.getElementById("logContainer");
            const p = document.createElement("p");
            p.textContent = message;
            logContainer.appendChild(p);
        }
    </script>
</body>
</html>